{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT + LoRA\n",
    "\n",
    "Fine-tune a DistilBERT classifier with LoRA (Low-Rank Adaptation) for toxic comment detection.\n",
    "Same pipeline as the full fine-tuning baseline, but only a fraction of parameters are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DATA_DIR = Path(\"../../data\")\n",
    "MODEL_DIR = DATA_DIR / \"models/transformer_lora\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.insert(0, str(Path(\"../..\" ).resolve()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / \"train_dataset.csv\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Toxicity rate: {df['y'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].values\n",
    "y = df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tune with LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) freezes the pretrained weights and injects trainable\n",
    "rank-decomposition matrices into the attention layers, drastically reducing\n",
    "the number of trainable parameters while preserving quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LoRATransformerClassifier\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "clf = LoRATransformerClassifier(\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    max_length=128,\n",
    "    batch_size=64,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train.tolist(), y_train,\n",
    "    epochs=3, batch_size=64, lr=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test.tolist())\n",
    "print(classification_report(y_test, y_pred, target_names=['Safe', 'Toxic']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (LoRA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save(str(MODEL_DIR))\n",
    "\n",
    "with open(MODEL_DIR / \"test_data.pkl\", 'wb') as f:\n",
    "    pickle.dump({'X_test': X_test, 'y_test': y_test}, f)\n",
    "\n",
    "print(\"Models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Model Using Our Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if 'model' in sys.modules:\n",
    "    importlib.reload(sys.modules['model.models'])\n",
    "    importlib.reload(sys.modules['model'])\n",
    "\n",
    "from model import LoRATransformerClassifier\n",
    "\n",
    "loaded_model = LoRATransformerClassifier(model_dir=str(MODEL_DIR))\n",
    "\n",
    "test_texts = [\"You are stupid\", \"Have a nice day\"]\n",
    "predictions = loaded_model.predict(test_texts)\n",
    "probas = loaded_model.predict_proba(test_texts)\n",
    "\n",
    "print(\"Test predictions:\")\n",
    "for text, pred, proba in zip(test_texts, predictions, probas):\n",
    "    print(f\"  '{text}' -> {pred} (prob: {proba[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Evaluation with get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_size = 1000\n",
    "X_test_subset = X_test[:eval_size].tolist()\n",
    "y_test_subset = y_test[:eval_size]\n",
    "\n",
    "results = loaded_model.get_metrics(\n",
    "    X_test_subset,\n",
    "    y_test_subset,\n",
    "    n_latency_runs=50\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION RESULTS (LoRA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nQuality Metrics:\")\n",
    "for metric, value in results['quality'].items():\n",
    "    print(f\"  {metric:15s}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm_array = np.array(results['confusion_matrix'])\n",
    "print(f\"  [[TN={cm_array[0,0]:5d}, FP={cm_array[0,1]:5d}]\")\n",
    "print(f\"   [FN={cm_array[1,0]:5d}, TP={cm_array[1,1]:5d}]]\")\n",
    "\n",
    "print(f\"\\nLatency Metrics:\")\n",
    "for metric, value in results['latency'].items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nThroughput: {results['throughput_samples_per_sec']:.2f} samples/sec\")\n",
    "print(f\"Peak Memory: {results['peak_memory_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "quality_data = results['quality']\n",
    "axes[0].bar(quality_data.keys(), quality_data.values(), color='steelblue')\n",
    "axes[0].set_title('Quality Metrics (LoRA)', fontweight='bold')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, (k, v) in enumerate(quality_data.items()):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "cm_array = np.array(results['confusion_matrix'])\n",
    "sns.heatmap(cm_array, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Safe', 'Toxic'], yticklabels=['Safe', 'Toxic'])\n",
    "axes[1].set_title('Confusion Matrix (LoRA)', fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "latency_data = results['latency']\n",
    "latency_values = [latency_data['latency_mean_ms'], latency_data['latency_std_ms'],\n",
    "                  latency_data['latency_min_ms'], latency_data['latency_max_ms']]\n",
    "latency_labels = ['Mean', 'Std', 'Min', 'Max']\n",
    "axes[2].bar(latency_labels, latency_values, color='coral')\n",
    "axes[2].set_title('Latency (ms per sample)', fontweight='bold')\n",
    "axes[2].set_ylabel('Milliseconds')\n",
    "for i, v in enumerate(latency_values):\n",
    "    axes[2].text(i, v + 0.01, f'{v:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel Summary (LoRA):\")\n",
    "print(f\"  Throughput: {results['throughput_samples_per_sec']:.0f} samples/sec\")\n",
    "print(f\"  Avg Latency: {latency_data['latency_mean_ms']:.2f} ms/sample\")\n",
    "print(f\"  F1 Score: {quality_data['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = MODEL_DIR / \"evaluation_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation results saved to {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}